#import "./template.typ":report
#import "@preview/codelst:2.0.2": sourcecode, sourcefile, codelst
#import "@preview/showybox:2.0.3": showybox

#show: report.with(
  title: "Συστήματα Παράλληλης Επεξεργασίας",
  subtitle: "Εργαστηριακή Αναφορά",
  authors: ("Λάζου Μαρία-Αργυρώ (el20129)",
            "Σπηλιώτης Αθανάσιος (el20175)"),
  team : "parlab09",
  semester: "9ο Εξάμηνο, 2024-2025",
)

#let my_sourcefile(file, lang: auto, ..args) = {
  showybox(
    frame: (
      body-color: rgb(245, 245, 245), // Light background (like VSCode light theme)
      border-color: rgb(200, 200, 200), // Light gray border
      title-color: rgb(200, 200, 200), // Darker gray title for contrast
      radius: 5pt, // Rounded corners
      thickness: 1pt, // Thin border for a clean look
    ),
    breakable: true,
    width: 100%, // Fill the available width
    align: center,
    color: rgb(80, 80, 80), // Dark gray text (like VSCode)
    title: grid(
      columns: (1fr, 1fr), // Two equal-width columns for title and language
      gutter: 1em, // Space between columns
      text(
        font: "DejaVu Sans Mono", // Monospaced font for code
        size: 0.75em,
        fill: rgb(80, 80, 80), // Dark gray title text
        weight: 600,
        file.split("/").at(-1)
      )
    ),
  )[
    #set text(size: 0.85em, font: "DejaVu Sans Mono", fill: rgb(10, 10, 10)) // Black text for the code
    #sourcefile(read(file), file: file, lang: lang, ..args, frame: none)
  ]
}

#let bash_box(file, lang: auto, ..args) = {
  show table.cell.where(y: 0): set text(weight: "regular")

  showybox(
    frame: (
      body-color: rgb("#111"),
      thickness: 0.6pt,
    ),
    breakable: false,
    width: 100%,
    align: center,
  )[
    #set text(
      font: "DejaVu Sans Mono",
      size: 0.8em,
      fill: lime, // Green text color for terminal style
    )
    #sourcefile(
      read(file),
      file: file,
      // lang: "bash",
      ..args,
      frame: none,
      numbering: none,
    )
  ]
}

#let bordered_text(file) = {
  // Read the file content
  let file_content = read(file)
  
  // Display the file content inside the box
  showybox(
    frame: (
      border-color: black,         
      border-thickness: 1pt,       
      radius: 4pt,                 
      thickness: 1pt,             
    ),
    breakable: true,
    width: 100%,                  
    align: center,
 
    text(size: 12pt, fill: black)[#file_content]  
    )
}

/****************************
* Document body, start here.*
*****************************/

= Conway's GameofLife
\
 === Υλοποίηση
Για την παραλληλοποίηση του αλγορίθμου τροποποίησαμε τον κώδικα που δίνεται προσθέτοντας απλώς το \#pragma directive στο κύριο loop για τα (i,j) του body: 

#my_sourcefile("../a1/Game_Of_Life.c",
highlighted: (63,),
  highlight-color:  rgb(85, 0, 170).lighten(60%),) 
\

Για την μεταγλώττιση και εκτέλεση στον scirouter χρησιμοποίησαμε το ακόλουθα scripts :

#bash_box("../a1/make_on_queue.sh")
#bash_box("../a1/omp_run_on_queue.sh")
\
=== Aποτελέσματα Μετρήσεων:
\
#bordered_text("../a1/omp_gameoflife_all.out")
\

=== Γραφική Απεικόνιση και Παρατηρήσεις

#image("../a1/grid64.svg")
Παρατηρούμε ότι για μικρό μέγεθος grid (με συνολική απαίτηση μνήμης 4*64*64bytes =
16KB), δεν υπάρχει ομοιόμορφη κλιμάκωση της επίδοσης με αύξηση των νημάτων από 4
και πάνω. Bottleneck κόστους θα θεωρήσουμε την ανάγκη συγχρονισμού των threads και
το overhead της δημιουργίας τους συγκριτικά με τον φόρτο εργασίας που τους ανατίθεται
(granularity).
\
\
\
#image("../a1/grid1024.svg")
Για μέγεθος grid με συνολική απαίτηση μνήμης 4*1024*1024 bytes = 4ΜB, η επίδοση
βελτίωνεται ομοιόμορφα και ανάλογα με το μέγεθος των νημάτων . Εικάζουμε, λοιπόν, πως
η cache χωράει ολόκληρο το grid ώστε το κάθε νήμα να μην επιβαρύνει την μνήμη με loads
των αντίστοιχων rows, o φόρτος εργασίας είναι ισομοιρασμένος στους workers και το
κόστος επικοινωνίας αμελητέο. Συνεπώς, προκύπτει perfect scaling.

#pagebreak() 

#image("../a1/grid4096.svg")
Για μεγάλο grid (με συνολική απαίτηση μνήμης 4*4096*4096 bytes = 64ΜΒ), η κλιμάκωση
παύει να υφίσταται για περισσότερα από 4 νήματα. Bottleneck κόστους εδώ θεωρούμε το
memory bandwidth. Επειδή ολόκληρο το grid δεν χωράει στην cache, δημιουργούνται
misses όταν ξεχωριστά νήματα προσπαθούν να διαβάσουν ξεχωριστές γραμμές του
previous. Σε κάθε memory request αδειάζουν χρήσιμα data για άλλα νήματα, φέρνοντας τις
δικές τους γραμμές και στο μεταξύ oι υπολογισμοί stall-άρουν.


#pagebreak() 

 === Bonus 

Δύο ενδιαφέρουσες ειδικές αρχικοποιήσεις του ταμπλό είναι το pulse και το gosper glider
gun, για τις οποίες η εξέλιξη των γενιών σε μορφή
κινούμενης εικόνας φαίνεται με μορφή gif παρακάτω: 

#align(center)[
#image("../a1/glider_gun.gif", width:75%)
#emph("glider_gun animation")
\
\
#image("../a1/pulse.gif", width: 75%)
#emph("pulse animation")
]

#pagebreak() 

=== Πράρτημα

Για την εξαγωγή των γραφικών παραστάσεων χρησιμοποιήθηκε ο κώδικας σε Python που ακολουθεί:

#my_sourcefile("../a1/plots.py")

#pagebreak() 

= KMEANS
\
== 1) Shared Clusters
=== Υλοποίηση
Για την παραλληλοποίηση της συγκεκριμένης έκδοσης χρησιμοποιήσαμε το parallel for directive του οmp και για την αποφυγή race conditions τα omp atomic directives. Αυτά εμφανίζονται όταν περισσότερα από 1 νήματα προσπαθούν να ανανεώσουν τιμές στους shared πίνακες newClusters και newClusterSize σε indexes, τα οποία δεν είναι μοναδικά για το καθένα, καθώς και στην shared μεταβλητή delta. Για αυτήν, προσφέρεται η χρήση reduction και εδώ μπορεί να αγνοηθεί εντελώς, αφού η σύγκλιση του αλγορίθμου καθορίζεται από τον πολύ μικρό αριθμό των επαναλήψεων(10). Ωστόσο, χρησιμοποιούμε atomic για ορθότητα της τιμής του και για παρατήρηση με βάση το μεγαλύτερο δυνατό overhead.

#my_sourcefile("../a2/kmeans/omp_naive_kmeans.c",
highlighted: (89,96,106,112),
  highlight-color:  rgb(85, 0, 170).lighten(60%),) 
\
Απεικονίζουμε παρακάτω τα αποτελέσματα των δοκιμών στον sandman για τις διάφορες τιμές της environmental variable OMP_NUM_THREADS:
\
#image("../a2/kmeans/results/fig0.png")

Παρατηρούμε πως ο αλγόριθμος δεν κλιμακώνει καθόλου καλά από 8 και πάνω νήματα εξαιτείας της σειριποίησης των εγγραφών, ολοένα και περισσότερων νημάτων που επιβάλλει η omp atomic, και της αυξανόμενης συμφόρησης στο bus κατά την απόκτηση του lock.

#pagebreak() 

== Εκμετάλλευση του GOMP_CPU_AFFINITY 
\

Με την χρήση του environmental variable GOMP_CPU_AFFINITY και στατικό scheduling κάνουμε pin νήματα σε πυρήνες(εφόσον δεν υπάρχει ανάγκη για περίπλοκη δυναμική δρομολόγηση). Έτσι, δεν σπαταλάται καθόλου χρόνος σε flash πυρήνων και αχρείαστη μεταφορά δεδομένων από πυρήνα σε άλλον. 
\
Για την υλοποίηση τροποποίησαμε κατάλληλα το script υποβολής στον sandman και προσθέσαμε την παράμετρο *schedule (static)* στο parallel for. 

\
=== Aποτελέσματα
#image("../a2/kmeans/results/fig1.png")

Παρατηρούμε σημαντική βελτίωση στην κλιμάκωση μέχρι 8 νήματα, όμως μετά σταματάει να κλιμακώνει ο αλγόριθμος λόγω της δομής που έχει ο sandman. Για 16 νήματα και πάνω δεν μπορούμε να τα κάνουμε pin στο ίδιο cluster, οπότε δεν μοιράζονται τα νήματα την ίδια L3 cache και υπάρχει συνεχής μεταφορά δεδομένων των shared πινάκων και bus invalidations λόγω του cache coherence protocol. Aκόμη τα L3 misses κοστίζουν ξεχωριστά για κάθε cluster. Εαν αξιοποιήσουμε τo hyperthreading και κάνουμε pin τα threads 8-15 στους cores 32-39 που πέφτουν μέσα στο cluster 1, μπορούμε να μειώσουμε σημαντικά τον χρόνο για τα 16 νήματα. Από εκεί και πέρα η κλιμάκωση σταματάει. Παραθέτουμε το τελικό script υποβολής ακολούθως:
\
#bash_box("../a2/kmeans/run_with_gomp.sh")
\
=== Aποτελέσματα
#image("../a2/kmeans/results/fig7.png")

#pagebreak() 

== 2) Copied Clusters & Reduce

=== Yλοποίηση
Μοιράζουμε σε κάθε νήμα ένα διαφορετικό τμήμα των πινάκων newClusters, newClusterSize, οπότε τα δεδομένα γίνονται private, δεν υπάρχουν race conditions αλλά απαιτείται reduction (με πρόσθεση) στο τέλος για το τελικό αποτέλεσμα (η οποία πραγματοποιείται εδώ από 1 νήμα). 

// TODO add code here !
#my_sourcefile("../a2/kmeans/omp_reduction_kmeans1.c",
highlighted: (76,77,85,86,87,88,89,106,107,108,109,110,111,112,113,115,117,120,138,139,140,149,150,151,152,153,154,155),
   highlight-color:  rgb(85, 0, 170).lighten(60%),) 

=== Aποτελέσματα
#image("../a2/kmeans/results/fig2.png")

Παρατηρούμε τέλεια κλιμάκωση μέχρι και τα 32 νήματα και αρκετά καλή και στα 64 εφόσον δεν εισάγουμε overheads συγχρονισμού και η σειριακή ενοποίηση (reduction) δεν είναι computational intensive για να καθυστερεί τον αλγόριθμο.
\
\
=== Δοκιμές με μικρότερο dataset

Τα αποτελέσματα δεν είναι ίδια για άλλα μεγέθη πινάκων. Συγκεκριμένα για το επόμενο configuration παρατηρούμε τα εξής:

 #image("../a2/kmeans/results/fig3.png") 

Κυρίαρχo ρόλο για αυτήν την συμπεριφορά αποτελεί το φαινόμενο false sharing, που εμφανίζεται σε μικρά datasets (εδώ κάθε object έχει μόνο 1 συντεταγμένη!), όταν σε ένα cache line καταφέρνουν να χωρέσουν παραπάνω από 1 objects και σε κάθε εγγραφή γίνονται πάρα πολλά περιττά invalidations. Mια λύση είναι το padding όμως έχει memory overhead και δεν προτιμάται.

=== First-touch Policy 
Προς αποφυγή των παραπάνω εκμεταλλευόμαστε την πολιτική των linux κατά το mapping των virtual με physical addresses. H δέσμευση φυσικής μνήμης πραγματοποιείται κατά την 1η εγγραφή του αντικειμένου (η calloc το εξασφαλίζει γράφοντας 0 ενώ η malloc όχι), οπότε εάν το κάθε νήμα γράψει ξεχωριστά στο κομμάτι του πίνακα που του αντιστοιχεί (ουσιαστικά παραλληλοποιώντας την αντιγραφή των shared πινάκων) θα απεικονιστεί στην μνήμη του αυτό και μόνο. 
=== Υλοποίηση 
// TODO add code here !
#my_sourcefile("../a2/kmeans/omp_reduction_kmeans.c",
highlighted: (75,76,99,100,101,102,103,104,105,106,107,108,109,110,111,112),
    highlight-color:  rgb(85, 0, 170).lighten(60%),) 


=== Αποτελέσματα 

#image("../a2/kmeans/results/fig4.png")

Υπάρχει σαφής βελτίωση και καλή κλιμάκωση μέχρι τα 32 νήματα ακόμα και σε σχέση με την ιδανική εκτέλεση του σειριακού αλγορίθμου. Ο καλύτερος χρόνος σε αυτό το ερώτημα είναι 0.4605s στα 32 νήματα! 

=== Numa-aware initialization

Με βάση όσα αναφέρθηκαν για το pinning σε cores και την πολιτική first-touch, η αρχικοποίηση των shared πινάκων μπορεί να γίνει και αυτή ατομικά από κάθε νήμα σε ένα private τμήμα αυτού. Για την υλοποίηση προσθέτουμε το omp parallel for directive με στατική δρομολόγηση. Aυτή είναι απαραίτητη, ώστε τα νήματα που θα βάλουν τους τυχαίους αριθμούς στα objects, να είναι τα ίδια νήματα με αυτά που θα τα επεξεργαστούν στην main.c με σκοπό να είναι ήδη στις caches και να μην χρειάζεται να τα μεταφέρνουν από την κύρια μνήμη ή από άλλα νήματα. 
=== Υλοποίηση 
Τροποποιούμε το file_io.c που δίνεται : 
// TODO add code here !
#my_sourcefile("../a2/kmeans/file_io.c",
highlighted: (28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50),
    highlight-color:  rgb(85, 0, 170).lighten(60%),) 
)



\
=== Αποτελέσματα 

#image("../a2/kmeans/results/fig5.png")

Παρατηρούμε καλύτερη κλιμάκωση μέχρι τα 32 νήματα με χρόνο 0.2667s!
Το κυρίαρχο bottleneck σε αυτήν την περίπτωση είναι το overhead της δημιουργίας των νημάτων.

\
\
Tέλος με όλες τις προηγούμενες αλλαγές δοκιμάζουμε ξανά το μεγάλο dataset που είχαμε στην αρχή:

#image("../a2/kmeans/results/fig6.png")

Παρατηρούμε πως υπάρχει τέλεια κλιμάκωση του αλγορίθμου. Οπότε bottleneck θα μπορούσε να θεωρηθεί το computive intensity για κάθε object.

#pagebreak() 

= FLOYD WARSHALL

== 1) Recursive 

=== Υλοποίηση

\
Δημιουργούμε ένα παράλληλο section κατά την πρώτη κλήση, αφού έχουμε ενεργοποιήσει την επιλογή για nested tasks μέσω τηw  omp_set_nested(1). (*Μπορούμε να το θέσουμε και ως environmental variable (OMP_NESTED=TRUE, OMP_MAX_ACTIVE_LEVELS=64) *)
Για την διατήρηση των εξαρτήσεων κατά τον υπολογισμό των blocks (A11) -> (A12 A21) -> A22 και αντιστρόφως, τοποθετούμε κατάλληλα barriers έμμεσα με τα taskwait directives. 

#my_sourcefile("../a2/FW/fw_sr.c",
highlighted: (13,43,45,49,50,51,52,53,54,55,95,97,102,104,106),
    highlight-color:  rgb(85, 0, 170).lighten(60%),) 

\
#pagebreak()

Πειραματιστήκαμε σχετικά με την βέλτιστη τιμή του BSIZE τρέχοντας τις προσομοιώσεις που ακολουθούν. Διαισθητικά η optimal τιμή οφείλει να εκμεταλλεύεται πλήρως το cache size και δεδομένου ότι έχουμε τετράγωνο grid για 1 recursive call που δημιουργεί 4 sub-blocks μεγέθους B θα είναι Βopt = sqrt(cache size). Για τα πειράματα χρησιμοποιήσαμε το ακόλουθο script: 
\
#bash_box("../a2/FW/run_on_queue.sh")
\

=== Aποτελέσματα 
\
#align(center)[
===  {N = 1024}
\
#image("../a2/FW/results/fig1024_16.png", width:60%)
#image("../a2/FW/results/fig1024_32.png", width:60%)
#image("../a2/FW/results/fig1024_64.png", width:60%)
#image("../a2/FW/results/fig1024_128.png", width:60%)
#image("../a2/FW/results/fig1024_256.png", width:60%)
\

=== {N = 2048}
\
#image("../a2/FW/results/fig2048_16.png", width:60%)
#image("../a2/FW/results/fig2048_32.png", width:60%)
#image("../a2/FW/results/fig2048_64.png", width:60%)
#image("../a2/FW/results/fig2048_128.png", width:60%)
#image("../a2/FW/results/fig2048_256.png", width:60%)

=== { N = 4096 }
\
#image("../a2/FW/results/fig4096_16.png", width:60%)
#image("../a2/FW/results/fig4096_32.png", width:60%)
#image("../a2/FW/results/fig4096_64.png", width:60%)
#image("../a2/FW/results/fig4096_128.png", width:60%)
#image("../a2/FW/results/fig4096_256.png", width:60%)
]

Kαταλήξαμε πως η ιδανική τιμή είναι Β=64 και ο καλύτερος χρόνος που πετύχαμε χρησιμοποιώντας αυτήν για 4096 μέγεθος πίνακα ήταν 10.4486 με 16 threads. Από το σημείο αυτό και έπειτα ο αλγόριθμος δεν κλιμακώνει και φανερώνει την αδυναμία του χάρη στην αναδρομή.

#pagebreak() 

== 2) TILED

=== Υλοποίηση 

Φτιάχνουμε 1 παράλληλo section με κατάλληλα barriers, ώστε να υπολογίζεται πρώτα (single) το k-οστό στοιχείο στην διαγώνιο, έπειτα όσα βρίσκονται κατά μήκος του "σταυρού" που σχηματίζεται εκατέρωθεν αυτού, και τέλος τα blocks στοιχείων που απομένουν. Καθένα από τα στάδια 2 και 3 έχει 4 for loops που μπορούν να παραλληλοποιηθούν με parallel for και επειδή είναι ανεξάρτητα μεταξύ τους με παράμετρο nowait. Το collapse(2) πραγματοποιεί flattening για καλύτερη λειτουργία του parallel for για nested loops. Mε χρήση μόνο των παραπάνω επιτυγχάνουμε χρόνο εκτέλεσης 2.2 secs.  
\
Για περαιτέρω βελτίωση επιχειρήσαμε να χρησιμοποιήσουμε SIMD εντολές αρχικά μέσω του OpenMP με το αντίστοιχο directive και στην συνέχεια γράφοντας χειροκίνητα τις intrinsics εντολές για AVX μοντέλο που υποστηρίζει 4-size vector operations, καθώς διαπιστώσαμε ότι vector operations μεγαλύτερου μεγέθους (π.χ με 8 στοιχεία AVX2) δεν υποστηρίζεται στο εν λόγω μηχάνημα και λαμβάνουμε σφάλμα Illegal hardware instruction. Στην πρώτη εκδοχή λάβαμε συνολικό χρόνο εκτέλεσης 1.7secs. 
\
H χρήση των intrisincs απευθείας μας δίνει την δυνατότητα να εκμεταλλευτούμε πλήρως και την αρχιτεκτονική της κρυφής μνήμης μέσω loop unrolling. Συγκεκριμένα, αναγνωρίσαμε ότι το size του cacheline είναι 64bytes, συνεπώς χωράνε 16 integers, ή 4 vectors 4άδων σε όρους AVX. Άρα επιτυγχάνουμε μέγιστο locality exploitation κάνοντας unroll με παράγοντα 4 και αυξάνοντας το j κατά 16 σε κάθε iteration. Ακόμη, παρατηρούμε ότι τα στοιχεία Α[i][k] είναι ανεξάρτητα του j  και η φόρτωση αυτών των vectors μπορεί να γίνει στο εξωτερικό loop. O καλύτερος χρόνος εκτέλεσης που επιτύχαμε αυτήν την εκδοχή είναι *1.39 secs!*
\

#my_sourcefile("../a2/FW/fw_smd.c",
highlighted: (4,5,27,28,29,30,40,41,42,43,45,46,50,54,58,62,64,69,74,79,84,94,95,96,97,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153),
    highlight-color:  rgb(85, 0, 170).lighten(60%),) 

=== Aποτελέσματα 

#image("../a2/FW/results/tiled.png")

Παραθέτουμε αναλυτικά και τους βέλτιστους χρόνους:
#bordered_text("../a2/FW/results/smd.out")

#pagebreak() 

== Παράρτημα 
\
Για την δημιουργία των γραφικών παραστάσεων χρημιοποίηθηκαν oι εξής κώδικες σε Python :

#my_sourcefile("../a2/kmeans/results/results.py")
\
\
#my_sourcefile("../a2/FW/results/plots.py")

#pagebreak()

= Αμοιβαίος Αποκλεισμός-Κλειδώματα

\
Στο συγκεκριμένο ερώτημα καλούμαστε να αξιολογήσουμε τους διαφορετικούς τρόπους υλοποίησης κλειδωμάτων για αμοιβαίο αποκλεισμό. 
\
Μας δίνονται έτοιμες όλες οι υλοποίησεις των κλειδωμάτων. Για την εκτέλεση του συγκεκριμένου data set (Size = 32, Coords = 16, Clusters = 32, Loops = 10) στον scirouter χρησιμοποιήσαμε το ακόλουθο script :

#bash_box("../a2_new/kmeans/run_on_queue.sh")

=== Τεχνικές συγχρονισμού

\
1) pthread_mutex_lock
\
Σε αυτήν την τεχνική χρησιμοποιείται ένα κλείδωμα αμοιβαίου αποκλεισμού και μεσολαβεί το λειτουργικό σύστημα σε περίπτωση αποτυχίας (context switch). Έτσι, επιτρέπει σε άλλες διεργασίες να τρέχουν μέχρι να ξυπνήσει από κάποιο κατάλληλο signal. Τότε, επιχειρεί εκ νέου να μπει στο κρίσιμο τμήμα. 
\
\
2) pthread_spin_lock
\
Σε αυτήν την τεχνική, χρησιμοποείται και πάλι ένα κλείδωμα όμως το κάθε νήμα εκτελεί busy waiting loop για την απόκτηση του. Έτσι, δεν επιτρέπει σε άλλο νήμα να τρέξει στην θέση του (εκτός φυσικά εάν περάσει χρόνος ίσος με το runtime quantum και το αποσύρει ο scheduler) σπαταλώντας ωφέλιμο CPU time. Αυτό το μοντέλο προσφέρεται για operations που μπλοκάρουν μόνο για λίγους κύκλους, δηλαδή ο χρόνος αναμονής είναι μικρότερος του context switching overhead.
\
3) tas_lock
\
Αυτή η τεχνική βασίζεται στην υποστήριξη από το υλικό και χρησιμοποιεί την ατομική εντολή test_and_set που προσφέρει το ISA. Θέτει *ταυτόχρονα* το κλείδωμα (μεταβλητή state) σε 1 και επιστρέφει την προηγούμενη τιμή της (μεταβλητή test). Αν η προηγούμενη τιμή είναι 0 τότε το νήμα δέσμευσε επιτυχώς το κλείδωμα. Κάθε νήμα που προσπαθεί να μπει στο κρίσιμο εκτελεί ένα dummy while loop με την tas. Σε κάθε iteration γράφει στην θέση μνήμης της state, που είναι *μοιραζόμενη*, και στέλνει cache line invalidation στα υπόλοιπα με βάση το πρωτόκολλο MESI για συνάφεια κρυφών μνημών. Συνεπώς, δημιουργείται υπερβολικά μεγάλη και περιττή συμφόρηση στο δίαυλο.
\
4) ttas_lock
\
Μοιάζει με την tas, ωστόσο το νήμα δεν γράφει απευθείας την state, αλλά επιχειρέι πρώτα να την διαβάσει (test). Εάν η τιμή της δεν είναι 0, παραμένει μέσα στο busy wit loop και μόλις διαβάσει τιμή 0, προσπαθεί να γράψει σε αυτήν (test_and_set). Τα διαδοχικά reads δεν κοστίζουν σε bandwidth αφού δεν στέλνουν κάποια ενημέρωση μέσω bus. O αριθμός των writes μειώνεται σημαντικά άρα και τα συνολικά invalidations. Περαιτέρω βελτίωση γίνεται με εκθετική οπισθοχώρηση (κατά το read και έτσι μειώνονται dummy CPU cycles και αποφεύγονται περισσότερα αποτυχημένα writes), αλλά δεν το εξετάζουμε σε αυτήν την άσκηση.
\
\
5) array_lock
\
Σε αυτήν την τεχνική κάθε νήμα έχει μια δική του μεταβλητή slot, ένα global πίνακα flag και που είναι τώρα το τέλος της ουράς. Κάθε φορά που προσπαθεί ένα νήμα να πάρει το κλείδωμα, παίρνει το τέλος της ουράς και κάνει ατομική αύξηση κατά 1, θέτει αυτό ως δικό του slot και περιμένει πότε θα γίνει true. Κάθε φορά που ένα νήμα αποδεσμεύει το κλείδωμα, ξαναθέτει το slot του ως false και κάνει το επόμενο true ώστε να πάρει το κλείδωμα αυτός που έχει το επόμενο slot. Αυτή η τεχνική έχει λιγότερη συμφόρηση στο δίαυλο γιατί κάθε νήμα κάνει πάντα 3 αλλαγές για να δεσμεύσει και να αποδεσμεύσει. Επίσης είναι δίκαιη, δηλαδή τα νήματα εκτελούν το κρίσιμο τμήμα με την ίδια σειρά που προσπάθησαν να το δεσμεύσουν. Ωστόσο, ένα νήμα πρέπει να περιμένει στην χειρότερη περίπτωση μια πλήρη περιστροφή του δακτυλίου ώστε να μπει στο κρίσιμο τμήμα, ακόμη και αν είναι το μοναδικό που το επιδιώκει.
\
\
6) clh_lock
\
Σε αυτήν την τεχνική κάθε νήμα έχει ένα κόμβο με ένα κλείδωμα. Κάθε φορά που προσπαθεί να δεσμεύσει το κλείδωμα βάζει το δικό του κλείδωμα να είναι 1, αλλάζει ατομικά τον δείκτη στο κόμβο που αναπαριστά το τέλος της ουράς στον εαυτό του και μετά περιμένει πότε το κλείδωμα του προηγούμενου θα γίνει 0. Αντίστοιχα, όταν αποδεσμεύει το κλείδωμα απλά θέτει το δικό του κλείδωμα σε 0. Το μεγάλο πλεονέκτημα αυτής της τεχνικής είναι πως είναι πολύ κλιμακώσιμη για αρκετά threads, καθώς υπάρχει μόνο 1 κοινή μεταβλητή για τα threads και όχι ολόκληρος πίνακας.
\
\
7) pragma omp critical
\ 
Θα αξιολογήσουμε και την επίδοση της βιβλιοθήκης του OpenMP για το κρίσιμο τμήμα.
\
\
8) pragma omp atomic
\
Θα αξιολογήσουμε επίσης και την επίδοση μέχρι μόνο 2 ατομικών εντολών και όχι κρίσιμου τμήματος, καθώς μπορεί να μην αλλάζουν τις ίδιες μεταβλητές 2 νήματα.

#pagebreak()

=== Aποτελέσματα 

#image("../a2/conc_ll/results/locks_total.png",width:95%)
#image("../a2/conc_ll/results/locks_perloop.png",width:95%)

=== Παρατηρήσεις

Γενικά με την αύξηση των threads τα race conditions είναι συχνότερα, οπότε οι συνολικοί χρόνοι εκτέλεσης ανεξαρτήτως μηχανισμού έχουν bottleneck το κόστος συγχρονισμού, όταν η λύση μας πάψει πια να είναι scalable (από τα 8 threads και πάνω όπως φαίνεται στο σχήμα).

\ *naive* \
H υλοποίηση naive χρησιμοποιεί atomic add για την εγγραφή στα arrays newClusterSize και newClusters. Συγκεκριμένα, για παράμετρο coords = 16, πραγματοποιεί 16 + 1 (για το newClusterSize) ατομικές εγγραφές. Aπό 8 threads και πάνω, προσεγγίζει καλύτερα από τις υπόλοιπες την no_sync η οποία δεν χρησιμοποιεί κανένα μηχανισμό συγχρονισμού οπότε παράγει λάθος αποτελέσματα) και χρησιμοποιείται μόνο ως σημείο αναφοράς βέλτιστου χρόνου. 

\ *spinlocks* \
Aπό 8 threads και πάνω οι μηχανισμοί #emph("tas") και #emph("ttas") δεν κλιμακώνουν εξαιτείας της συμφόρησης που προκαλούν στον δίαυλο με τα αλλεπάλληλα cache line invalidations, ειδικότερα όταν τα δεδομένα χρειάζεται να μεταφέρονται πλέον εκτός του NUMA cluster οπότε χρειάζεται να διατηρείται η συνάφεια και μεταξύ των L3 Caches. 
Aκόμη, όσο περισσότερα γίνονται τα νήματα τόσο αυξάνονται και οι αποτυχημένες προσπάθειες της test_and_set και στις δύο περιπτώσεις εξαιτείας της μεγάλης ζήτηςης του ίδιου lock. Η #emph("ttas") έχει ωστόσο καλύτερες επιδόσεις, αφού γλιτώνει κάποια περιττά writes, και μάλιστα για λιγότερα από 8 νήματα έχει την καλύτερη επίδοση.
Παρόμοια είναι και η συμπεριφορά του pthread_spinlock αφού όλα βασίζονται στην λογική των busy-wait loops σπαταλώντας CPU time. 
Με βάση το implementation στην glibc, η pthread_spin_lock χρησιμοποιεί ένα υβρίδιο των προηγούμενων 2. Στην 1η προσπάθεια, χρησιμοποιεί την ατομική εντολή atomic_exchange που θεωρεί ταχύτερη εάν παρέχεται από το υλικό και δεν καλεί την CAS (compare_and_swap). Σε περίπτωση αποτυχίας απλά διαβάζει την μεταβλητή όπως η #emph("ttas"). Παραθέτουμε τον αντίστοιχο κώδικα:
#my_sourcefile("../a2/others/pthread_spin_lock.c")

\ *array locks* \
Το array_lock και clh_lock είναι οι πιο scaleable μηχανισμοί, αφού δεν χαρακτηρίζονται από το overhead της atomic σε λιγότερα από 8 νήματα και από το overhead του προωτοκόλλου MESI για  περισσότερα από 8 νήματα.
Επειδή τα νήματα εισέρχονται στο κρίσιμο τμήμα με ένα καθορισμένο μοτίβο και όλα εν τέλει θα μπουν σε αυτό, κανένα slot στον δακτύλιο δεν μένει αδρανές, ενώ όσο τα νήματα περιμένουν την σειρά τους δεν πραγματοποιούν ανούσιες προβάσεις την μνήμη. Αντιθέτως, εάν το concurrency rate ήταν μικρότερο, ο μηχανισμός θα έπασχε από το overhead διάσχυσης ολόκληρου του δακτυλίου προκειμένου να ικανοποίησει λ.χ. ένα μόνο αίτημα.
Aντίστοιχα, για το chl_lock μειώνεται το contention για ένα κοινό global lock και κάθε νήμα εκτελεί spinlock στην μεταβλητή του προηγούμενου node. Συγκρούσεις εξακολουθούμε να έχουμε για την είσοδο στο τέλος της ουράς, παρόλα αυτά η διατήρηση μιας λίστας είναι πιο φθηνή από έναν circular buffer και φαίνεται πως γι' αυτό  πετυχαίνει καλύτερους χρόνους.

\ *mutex* \
Για λιγότερα από 8 νήματα το context switch είναι αρκετά ακριβό (τουλάχιστον για το συκεκριμένο configuration όπου απαιτεί 17 μόνο πράξεις στο κρίσιμο τμήμα που δεν είναι τόσο computational intense), όμως φαίνεται να υπερτερεί έναντι των spinocks για 16 και πάνω νήματα όπου το bus traffic γίνεται αφόρητο. H υλοποίηση του omp critical φαίνεται ότι συνδέεται με την χρήση ενός mutex, ωστόσο έχει χειρότερη επίδοση από το να το καλέσουμε explicitly, που διακιολογούμε εφόσον παρέχει ένα higher level abstraction άρα και επιπλέον overheads. Συγκεκριμένα, διατηρεί μέσω του global context manager ένα mapping για named crirtical regions το οποίο εισάγει μια πολυπλοκότητα, ενώ τα omp directives απαιτούν κάθε φορά και την κλήση της libomp. Παραθέτουμε τον αντίστοιχο κώδικα:
#my_sourcefile("../a2/others/gomp_critical.c")
\
\
Τέλος, όταν έχουμε 1 μόνο νήμα, δεν μπλοκάρει σε καμία απόπειρα απόκτησης του lock γι'αυτό όλες οι υλοποιήσεις είναι καλύτερες από την naive, (tas,ttas,array,clh εκτελούν ακριβώς τις ίδεις εντολές - ανάγονται σε 1 ανάγνωση και 1 εγγραφή) και η mutex υπερτερεί γιατί δεν πραγματοποιεί κανένα context switch.
\
\
*#emph("Σημείωση")* : Αν χρησιμοποιούσαμε hyperthreading για τα 16 νήματα, δηλαδή βάζαμε τα 8 τελευταία νήματα εκτός των 64 λογικών, θα δούμε κλιμάκωση και για 16 νήματα όπως φαίνεται παρακάτω :
#image("../a2_new/kmeans/results/kmeans_results_hyper.png")


#pagebreak()

= Ταυτόχρονες Δομές δεδομένων
\
Σε αυτό το ερώτημα εξετάζουμε πως κλιμακώνουν διάφορες ταυτόχρονες υλοποιήσεις για μια απλά συνδεδεμένη λίστα. 
\
Οι ταυτόχρονες υλοποιήσεις που θα εξετάσουμε είναι οι εξής :
\
\
=== 1) Coarse-grain locking
\
Σε αυτήν την υλοποίηση υπάρχει ένα γενικό κλείδωμα για όλη την δομή. Για κάθε προσθήκη ή αφαίρεση στοιχείου στη λίστα, το νήμα προσπαθεί να δεσμεύσει το κλείδωμα και να κάνει την κατάλληλη αλλαγή. Είναι πολύ απλό στην υλοποίηση, όμως δεν θα κλιμακώσει καθόλου καθώς όλοι περιμένουν το ίδιο κλείδωμα και δεν εκμεταλλευόμαστε καθόλου παραλληλία σε ανεξάρτητα τμήματα της λίστας.
\
\
=== 2) Fine-grain locking 
\
Σε αυτήν την υλοποίηση υπάρχει ένα κλείδωμα για κάθε στοιχείο της λίστας. Ο τρόπος διάσχισης είναι hand-over-hand locking δηλαδή ένα νήμα προσπαθεί να δεσμεύσει τον επόμενο, όταν τα καταφέρει, αφήνει τον προηγούμενο. Αυτό είναι αναγκαστικό προς αποφυγή deadlock, εφόσον για ένα operation απαιτούνται κλειδώματα σε 2 στοιχεία (pred, curr) και θα δημιουργούνταν πρόβλημα αν 2 νήματα επιχειρούσαν να αλλάξουν 2 γειτονικά nodes και προσπαθούσαν να πάρουν τα κλειδώματα με αντίθετη σειρά. Μπορεί να δουλέψει καλύτερα από την coarse grain σε συγκεκριμένες περιπτώσεις, αλλά το σημαντικότερο πρόβλημα είναι πως αν ένα νήμα θέλει να αλλάξει κάτι που βρίσκεται νωρίς στη λίστα, μπλοκάρει όλα τα άλλα νήματα που θέλουν να ψάξουν ή αλλάξουν κάτι που είναι πιο μετά στη λίστα.
\
\
=== 3) Optimistic synchronization
\
Σε αυτήν την υλοποίηση ένα νήμα για κάθε αλλαγή, βρίσκει τον προηγούμενο και τον επόμενο προς αλλαγή, προσπαθεί να τους δεσμεύσει, ελέγχει αν η δομή είναι ακόμη συνεπής(δηλαδή είναι προσβάσιμοι και διαδοχικοί) και κάνει την αλλαγή. Η contains στη συγκεκριμένη υλοποίηση χρησιμοποιεί επίσης κλειδώματα αν και δεν χρειάζεται. Το κύριο πρόβλημα αυτής της υλοποίησης είναι πως η validate διατρέχει όλη την λίστα για να επιβεβαιώσει την συνέπεια και αυτό είναι πάρα πολύ χρονοβόρο.
\
\
=== 4) Lazy synchronization
Σε αυτήν την υλοποίηση προσθέτουμε στη δομή μια boolean μεταβλητή που δείχνει αν ο κόμβος βρίσκεται στη λίστα ή έχει διαγραφεί. Η contains διατρέχει τη λίστα χωρίς να κλειδώνει και ελέγχει αυτήν την boolean μεταβλητή, οπότε είναι wait-free. Η validate δεν διατρέχει την λίστα, αλλά κάνει τοπικούς ελέγχους στον προηγούμενο και επόμενο κόμβο, δηλαδή ελέγχει αν ανήκουν στη δομή και οι 2, με την επιπλέον μεταβλητή, και ο next του προηγούμενου είναι ο τωρινός. Η add/remove κάνουν πρώτα λογική και μετά φυσική αλλαγή των κόμβων.
\
\
=== 5) Non-blocking
\
Σε αυτήν την υλοποίηση προσπαθούμε να αφαίρουμε τελείως την ανάγκη για κλειδώματα και να χρησιμοποιήσουμε τις ατομικές εντολές που μας δίνει το instruction set του εκάστοτε επεξεργαστή. Η κεντρική ιδέα είναι να χειριστούμε την boolean μεταβλητή marked και το πεδίο next σαν μία μεταβλητή. Κάνει ατομικό σύνθετο έλεγχο και αλλαγή με 1 εντολή compare and set. Έτσι, η διαγραφή κάνει με 1 εντολή validate και λογική διαγραφή και 1 μόνο προσπάθεια φυσικής διαγραφής. Η find/contains είναι αυτή που εξετάζει αν υπάρχει στοιχείο που έχει διαγραφεί λογικά και όχι φυσικά και το αναλαμβάνει εκείνη. Η προσθήκη αναγκαστικά ξαναπροσπαθεί μέχρι να τα καταφέρει, για να είναι συνεπής η δομή.

\

Μας δίνονται έτοιμες όλες οι παραπάνω ταυτόχρονες υλοποιήσεις. Για την ζητούμενη εκτέλεση, το σειριακό πρόγραμμα εκτελέστηκε μόνο με 1 thread αλλιώς θα υπάρχει πρόβλημα, για 128 νήματα χρησιμοποιήθηκε oversubscription. Δηλαδή η μεταβλητή MT_CONF τέθηκε σε 0,1,...63,0,1,...63 , ώστε να δημιουργηθούν και να γίνουν pinned 128 νήματα σε συγκεκριμένους πυρήνες. Επειδή οι λογικοί πυρήνες του sandman είναι 64, το scheduling των νημάτων πλέον το αναλαμβάνει το λειτουργικό και το software και όχι το ίδιο το υλικό, όπως όταν χρησιμοποιούμε hyperthreading.
\
\
#bash_box("../a2_new/conc_ll/run_on_queue.sh")

\

== Αποτελέσματα
\
Παρουσιάζονται τα Kops/sec με την μορφή line plots ανά κάθε διαφορετικό workload configuration.

\

H serial εκδοχή μας δείχνει την δυναμική του κάθε core, δεν χρησιμοποιεί παραλληλισμό ούτε κλειδώματα και έχει σταθερό throughput ανεξάρτητα από το πλήθος των νημάτων γι'αυτό και την θεωρούμε ως σημείο αναφοράς.
Tα queries add / delete είναι υπολογιστικά ισοδύναμα, οπότε μπορούμε να εξάγουμε το συνολικό ποσοστό τους έναντι των search για την ανάλυσή μας.
\
#align(center)[
#image("../a2_new/conc_ll/results/conc_ll_1024_100_0_0.png", width:85%)
#image("../a2_new/conc_ll/results/conc_ll_8192_100_0_0.png",width:85%)
]

\ *Workload 100/0/0* \

Στην περίπτωση που το workload αποτελείται εξ'ολοκλήρου από search queries, oι *coarse-grain, fine-grain & και optimistic* εκδοχές δεν κλιμακώνουν καθόλου για κανένα μέγεθος λίστας αφού μπλοκάρονται όλες σε κάποιο κλείδωμα (είτε είναι global για όλη την δομή στην πρώτη περίπτωση, είτε στην αρχή της λίστας και περνιέται μέσω hand-over-hand για τις άλλες). Oυσιαστικά όλες οι προσβάσεις σειριοποιούνται, οπότε δεν κερδίζουμε τίποτα από τον παραλληλισμό, μονάχα το κόστος δημιουργίας των threads και υλοποίησης των κλειδωμάτων.
\

Aντίθετα, η *lazy* είναι wait-free στην αναζήτηση επομένως κερδίζουμε throughput χάρη στον παραλληλισμό χωρίς bottlenecks. Στα 64 - 128 νήματα η επίδοση μένει σταθερή γιατί το μηχάνημα έχει ήδη *100% utilization* (64 logical cores).
\
\
#align(center)[
#image("../a2_new/conc_ll/results/conc_ll_1024_80_10_10.png",width:85%)
#image("../a2_new/conc_ll/results/conc_ll_8192_80_10_10.png",width:85%)
]
\ *Workload 80/10/10* \

Προσθέτοντας μικρό ποσοστό add / delete βλέπουμε ότι η επιδόσεις όλων είναι καλύτερες, αφού μέρος αυτών των λειτουργιών είναι η συνάρτηση contains που πραγματαποιεί αναζήτηση αλλά με wait free τρόπο. Χειρότερη είναι η *fine-grain* ειδικότερα αν queries στο τέλος της λίστας έπονται από queries στην αρχή της και αναγκάζονται να μπλοκάρουν μέχρι να αποκτήσουν το lock με hand-over-hand τρόπο. Ακόμη, η διαδικασία αυτή εισάγει έξτρα πολυπλοκότητα μέσα από αλλεπάληλες κλήσεις lock / unlock μέχρι να φτάσει στους ζητούμενους κόμβους, οπότε καταλήγει να έχει χειρότερη επίδοση από την *coarse-grain*. H κατάσταση θα μπορούσε να αλλάξει εάν αντιστρέφαμε την σειρά των queries, όμως αυτά παράγονται τυχαία. 
\

H *optimistic* ακολουθεί την λογική readers-writer lock και διατρέχει την λίστα wait free κάνοντας μετά έναν έλεγχο συνέπειας της δομής (validate). Δεν κλιμακώνει όπως θα περίμεναμε, επειδή η validate έχει γραμμική πολυπλοκότητα και κυριαρχεί το κόστος να διατρέξει την λίστα από την αρχή. 
\

Αντίθετα, η *lazy* υλοποιεί την validate με σταθερό χρόνο, κοιτάζοντας απλά το valid bit για τους κόμβους pred, curr και από 16 threads και πάνω παρουσιάζει τεράστια κλιμάκωση. 
\

Τέλος, η *non-block* είναι εγγενώς wait free και αξιοποιεί packed εντολές που παρέχει το ISΑ και είναι optimal.
\

Η μείωση του throughput από τα 8 στα 16 οφείλεται στο ότι τόσο τα locks όσο και τα list data είναι shared στα threads και βγαίνουμε εκτός NUMA cluster. Δεν ισχύει το ίδιο και για μέγεθος 8192 όπου έτσι και αλλιώς η λίστα δεν χωράει ολόκληρη στην cache.
\
\
\
#align(center)[
#image("../a2_new/conc_ll/results/conc_ll_1024_20_40_40.png",width:85%)
#image("../a2_new/conc_ll/results/conc_ll_8192_20_40_40.png",width:85%)
]

#align(center)[
#image("../a2_new/conc_ll/results/conc_ll_1024_0_50_50.png",width:85%)
#image("../a2_new/conc_ll/results/conc_ll_8192_0_50_50.png",width:85%)
]


\ *Workload 20/40/40  & Workload 0/50/50* \
Aυξάνοντας παραπάνω το ποσοστό των add / delete η *lazy* χάνει σε throughput ενώ η *optimistic* κερδίζει στην περίπτωση του μικρού size=1024, και δεν παρουσιάζει αισθητές διαφορές για μεγάλο size=8192. Aυτό συμβαίνει επειδή τα conflicts για μεταβολή κοινών δεδομένων είναι σπανιότερα σε μεγάλο μέγεθος λίστας και εφόσον ο αριθμός των queries μένει σταθερός. Έτσι, το overhead των locking mechanisms μένει σχετικά σταθερό χάρη στο μικρό contention. 
\
\

\ *Σε όλα τα Workloads,* \
Το throughput στην πολύ μεγάλη λίστα είναι σημαντικά μικρότερο, αφού όλες οι λειτουργίες είναι γραμμικές ως προς το μέγεθος της λίστας και δεν μπορεί να αξιοποιηθεί πλήρως το data locality. Eπιπλέον, η lazy τα πάει χειρότερα στο μικρό μήκος από την non blocking. Η ειδοποιός διαφορά των δύο αυτών υλοποιήσεων είναι πως η lazy στις διαγραφές είναι επίμονη και προσπαθεί να αποκτήσει τα locks των κόμβων που απαιτούνται μέχρι να τα καταφέρει, ενώ η non blocking κάνει την λογική διαγραφή και 1 μόνο προσπάθεια φυσικής διαγραφής. Σε workloads με λίγα contains(που κάνει την φυσική αφαίρεση στην non blocking) και αρκετά add/remove η lazy κάνει συνέχεια και τις φυσικές διαγραφές ενώ η non blocking κάνει λίγες και γλυτώνει χρόνο.
\
\
Παρακάτω φαίνονται τα κανονικοποιημένα throughputs / thread_count σε σύγκριση με την serial εκδοχή και πως αυτά επηρεάζονται από το workload: 

#pagebreak()

#columns(2)[
  #image("../a2/conc_ll/results/conc_1024_1.png"),
  #image("../a2/conc_ll/results/conc_1024_4.png")
  \
  #image("../a2/conc_ll/results/conc_1024_16.png"),
  #image("../a2/conc_ll/results/conc_1024_64.png")
  \
  #image("../a2/conc_ll/results/conc_1024_2.png"),
  #image("../a2/conc_ll/results/conc_1024_8.png")
  \
  #image("../a2/conc_ll/results/conc_1024_32.png"),
  #image("../a2/conc_ll/results/conc_1024_128.png") 
]
  

#pagebreak()

\
\

#columns(2)[
#image("../a2/conc_ll/results/conc_8192_1.png"),
#image("../a2/conc_ll/results/conc_8192_4.png")
\
#image("../a2/conc_ll/results/conc_8192_16.png"),
#image("../a2/conc_ll/results/conc_8192_64.png")
\
#image("../a2/conc_ll/results/conc_8192_2.png"),
#image("../a2/conc_ll/results/conc_8192_8.png")
\
#image("../a2/conc_ll/results/conc_8192_32.png"),
#image("../a2/conc_ll/results/conc_8192_128.png")
]


#pagebreak()

== Παράρτημα
Για την δημιουργία των γραφικών παραστάσεων χρησιμοποίηθηκαν oι εξής κώδικες σε Python :

#my_sourcefile("../a2_new/kmeans/results/scraping_kmeans.py")
\
\
#my_sourcefile("../a2/conc_ll/results/conc_plt.py")

#pagebreak()

= Παραλληλοποίηση και βελτιστοποίηση αλγορίθμων σε επεξεργαστές γραφικών
\
\
Σκοπός αυτής της άσκησης είναι η υλοποίηση και η βελτιστοποίηση του αλγορίθμου Kmeans σε μια κάρτα γραφικών της Nvidia με την χρήση της Cuda. Αρχικά, υλοποιούμε μια naive προσέγγιση και έπειτα αξιολογούμε και συγκρίνουμε άλλες υλοποιήσεις που αφορούν τεχνικές βελτιώσεις σε GPUs.

== Naive 
Αρχικά υπολογίζουμε το global id στην συνάρτηση get_id() ως εξής : thread_block_size × block_id + local_thread_id. 
\
Έπειτα, στη συνάρτηση euclidean_distance() υπολογίζεται η ευκλείδια απόσταση μεταξύ δύο σημείων στον n-διάστατο χώρο. Ο πίνακας συντεταγμένων των αντικειμένων καθώς και των clusters είναι μονοδιάστατος. Οπότε, η συντεταγμένη j του cluster i είναι η cluster[i × numCoords + j] καθώς ο δισδιάστατος πίνακας θα είχε διαστάσεις [numClusters][numCoords], αντίστοιχα για τα objects. Με ένα for loop υπολογίζεται το άθροισμα όλων των διαφορών στο τετράγωνο. Δεν χρειάζεται να υπολογιστεί η ρίζα, καθώς οι αποστάσεις είναι μη αρνητικές και η ύψωση στο τετράγωνο δεν αλλάζει την μεταξύ τους διάταξη.
\
Η συνάρτηση find_nearest_cluster χρειάζεται να υπολογιστεί μόνο για τα threads που έχουν global id μικρότερο από τον αριθμό των objects, ώστε να μην βρεθεί εκτός ορίων πίνακα και κάθε thread να κάνει έναν υπολογισμό για ένα αντικείμενο. Για να βρεθεί το κοντινότερο cluster, υπολογίζεται αρχικά η απόσταση από το πρώτο cluster και έπειτα υπολογίζεται σειριακά η απόσταση για τα υπόλοιπα. Αν για κάποιο cluster, είναι μικρότερη από την ήδη υπάρχουσα, κρατάμε αυτό στη θέση του προηγούμενου. Ακόμη, γίνεται αύξηση του delta κατά 1, αν διαφέρει από το προηγούμενο κοντινότερο cluster. Χρειάζεται να γίνει με atomicAdd, ώστε να έχουμε σωστό συνολικό αποτέλεσμα καθώς είναι κοινό δεδομένο για όλα τα thread blocks και χρειάζεται κατάλληλος συγχρονισμός.
\
\
Ο αριθμός των thread blocks υπολογίζεται ως $"numClusterBlocks" = ("numObjects" + "blocksize" - 1) / "blocksize"$. Αυτό υπολογίζει την πράξη ceil του λόγου του αριθμού των αντικειμένων προς το blocksize, ώστε ακόμη και 1 παραπάνω thread να χρειάζεται να δημιουργηθεί καινούριο thread block.
\
Τέλος αντιγράφουμε τα clusters, memberships, delta χρησιμοποιώντας την εντολή cudaMemCpy με κατάλληλο μέγεθος και κατεύθυνση της αντιγραφής.
\
#my_sourcefile("../a3/cuda_kmeans_naive.cu",
  lang: "cpp",
  highlighted:(24,41,42,43,59,66,70,71,72,73,74,80,145,172,191,194), 
  highlight-color:  rgb(85, 0, 170).lighten(60%)
)
\
\
Μετρήθηκαν οι επιδόσεις της σειριακής και της naive έκδοσης για τα διάφορα blocksizes για {Size, Coords, Clusters, Loops} = {1024, 32, 64, 10}. Με βάση αυτές, προκύπτουν τα παρακάτω διαγράμματα χρόνου εκτέλεσης, speedup, χρόνου gpu-cpu-transfer αντίστοιχα.
\
\
#align(center)[
#image("../a3/plots_scraping/naive_exec_time.png",width:85%)
#image("../a3/plots_scraping/naive_speedup.png",width:85%)
#image("../a3/plots_scraping/naive_stacked_execution_time_blocksizes.png",width:85%)
]
\
\
Το speedup του kmeans για την naive έκδοση κυμαίνεται από 9 έως 12 ανάλογα το blocksize. Αυτή η επίδοση είναι αρκετά καλή για την πιο απλή υλοποίηση σε GPU. Ο αλγόριθμος kmeans όμως δεν είναι ιδανικός για GPU, καθώς κάνει αρκετά transfers και allocations. Επίσης το operational intensity δεν είναι το μέγιστο που μπορεί να υποστηρίξει η συγκεκριμένη GPU, καθώς δεν κάνει 6000 πράξεις ανά δεδομένο ώστε να αξίζει πλήρως η μεταφορά του.
\
\
Οι χρόνοι μεταφοράς και της CPU είναι προφανώς ίδιοι, οπότε οι αλλαγές του χρόνου της GPU μεταφράζονται άμεσα σε speedup. Το blocksize 32 έχει το καλύτερο, διότι έχει την μεγαλύτερη ευελιξία για το scheduling και κάθε thread block είναι ένα warp. Αφού τα objects είναι τελείως ανεξάρτητα μεταξύ τους για τον υπολογισμό του κοντινότερου cluster, χρησιμοποιούνται στο μέγιστο οι πόροι της GPU. Το blocksize 1024 έχει την μικρότερη ευελιξία, καθώς κάποιοι πόροι δεν αξιοποιούνται στο μέγιστο. Τέλος, το blocksize 238 παρόλο που δεν είναι πολλαπλάσιο του 32 και δεν αξιοποιεί όλα τα warps στο μέγιστο, αφήνονται αναξιοποίητα μόνο 18 threads, οπότε η διαφορά στο σύνολο δεν είναι τόσο μεγάλη και γι' αυτό παρατηρείται μια καλή επίδοση.
#pagebreak()

== Transpose
\
Οι μονοδιάστατοι πίνακες της GPU είναι πλέον column-based και όχι row-based όπως πριν. Οπότε, ο πίνακας clusters έχει διαστάσεις [numCoords][numClusters]. Η συντεταγμένη j του cluster i είναι η cluster[j × numClusters + i], καθώς πλέον ο πίνακας έχει για κάθε συντεταγμένη μία γραμμή από την τιμή της για κάθε cluster.
\
#my_sourcefile("../a3/code_parts_display/transpose_euclid_dist.cu",
  lang: "cpp",
  highlighted:(13,14,15,16), 
  highlight-color:  rgb(85, 0, 170).lighten(60%)
)
\
Η find_nearest_cluster μένει απαράλλακτη καθώς αλλάζει μόνο η δομή των δεδομένων. Οι πίνακες dimObjects, dimClusters, newClusters έχουν numCoords γραμμές ώστε να είναι column-based και γίνεται κατάλληλο allocation με την calloc_2d που μας παρέχεται. 
\
Πριν την εκτέλεση του αλγορίθμου χρειάζεται αντιγραφή των αντικειμένων σε μορφή column-based, οπότε το dimObjects[j][i] = objects[i][j]. Ο πίνακας objects όμως είναι μονοδιάστατος, ακολουθώντας την ίδια λογική μετατροπής objects[i][j] = objects[i × numCoords + j], καθώς numCoords γραμμές θα είχε ο αντίστοιχος δισδιάστατος πίνακας.
\
#my_sourcefile(
  "../a3/code_parts_display/transpose_allocation.cu",
  lang:"cpp",
  highlighted:(2,3,4,17),
  highlight-color:  rgb(85, 0, 170).lighten(60%)
)
\
Ο αριθμός των thread blocks δεν αλλάζει, καθώς και οι αντιγραφές πίσω στην CPU μετά τους υπολογισμούς της GPU. Αλλάζει όμως η αντιγραφή των clusters προς την GPU σε dimClusters, καθώς αυτά είναι τα column-based δεδομένα. Ακόμη, αφού ολοκληρωθεί η εκτέλεση του αλγορίθμου χρειάζεται οι συντεταγμένες των τελικών clusters να ξαναγίνουν row-based και κάνουμε την ανάποδη διαδικασία clusters[i][j] = clusters[i × numCoords + j] = dimClusters[j][i], το οποίο είναι ίδιο με την έκφραση στην αρχή του transpose με κατάλληλη αλλαγή δεικτών. Αναλυτικότερα, για πίνακα m γραμμών και n στηλών το στοιχείο i,j είναι το i × n + j ή αλλιώς το j × m + i.
\
#my_sourcefile(
  "../a3/code_parts_display/transpose_do_while.cu",
  lang:"cpp",
  highlighted:(9,71,72,73,74,75),
  highlight-color:  rgb(85, 0, 170).lighten(60%)
)
\
Επαναλάβαμε τις μετρήσεις για την transpose εκδοχή και παρακάτω παρουσιάζεται το διάγραμμα speedup σε σύγκριση με την naive εκδοχή.
\
#align(center)[
  #image("../a3/plots_scraping/transpose_naive_speedup.png",width:85%)
]
\
\
Παρατηρούμε πως το blocksize παίζει τελείως διαφορετικό ρόλο σε σχέση με την naive περίπτωση. Σε κάθε γραμμή βρίσκεται η τιμή της ίδιας συντεταγμένης για όλα τα objects, clusters. Οπότε, όταν ένα thread κάνει access μια συγκεκριμένη συντεταγμένη θα έρθουν στην cache οι τιμές της αντίστοιχης συντεταγμένες για επόμενα objects, clusters. Καθώς τα warps προχωράνε στις συντεταγμένες στην GPU, θα υπάρχουν στην L1 cache στοιχεία που θα χρησιμοποιήσουν τα επόμενα warps ή επόμενα thread του ίδιου warp. Έτσι, καλύπτεται το global memory latency, αφού περιμένουν πολύ λιγότερα warps στοιχεία για να κάνουν τους υπολογισμούς τους. Όταν έρχεται 1 cache line, θα καλύψει σίγουρα όσα threads υπολογίζουν συντεταγμένες που αντιστοιχούν σε αυτήν την cache line. Αυτό οδηγεί σε πολύ καλύτερη καλύψη των κενών χρόνων μεταξύ των warps και thread blocks, οπότε τα μεγάλα blocksizes έχουν σχεδόν όλα την ίδια επίδοση. Το blocksize 32 συνεχίζει να έχει αισθητά καλύτερα επίδοση λόγω ευελιξίας, όπως και στην naive εκδοχή.
#pagebreak()

== Shared
\
Χρειάζεται να επεκτείνουμε την find_nearest_cluster κατάλληλα. Η κοινή μνήμη έχει εμβέλεια στο thread block και είναι πολύ πιο γρήγορη από την global μνήμη. Γι' αυτό μπορούν να αποθηκευτούν σε αυτήν οι συντεταγμένες των clusters που τις χρειάζεται όλο το thread block, ώστε να έχει γρήγορη πρόσβαση σε αυτές. 
\
Το πρώτο βήμα είναι να αντιγραφούν οι συντεταγμένες των clusters στην shared memory για κάθε thread block. Τα clusters είναι 64 και το ελάχιστο blocksize είναι 32, οπότε σε αυτήν την περίπτωση θα πρέπει κάθε thread μέσα στο thread block να αντιγράψει 2 ολόκληρα clusters. Γενικότερα όμως είναι καλή πρακτική για να τρέχει για οποιοδήποτε μέγεθος blocksize και αριθμό cluster, να θεωρούμε πως κάθε thread έχει υπό την υπόλοιψή του παραπάνω από 1 cluster. 
\
Για να διαχωριστούν σωστά και ισάξια τα clusters κάθε thread ξεκινάει από το local id του και προχωράει με βήμα όσο το thread block, μέχρι να τελειώσει ο αριθμός των clusters. Υπενθυμίζεται πως τα clusters είναι column-based όπως και στην transpose εκδοχή. Οι υπολογισμοί των κοντινότερων clusters πρέπει να ξεκινήσουν αφού αντιγραφούν όλα τα clusters μέσα στο thread block. Οπότε, χρειάζεται ένας συγχρονισμός των νημάτων για να είναι σίγουρο πως θα έχει γίνει αυτό.
\
Τέλος, για να αξιοποιηθούν σωστά, στην θέση των deviceClusters μπαίνει ο πίνακας της διαμοιραζόμενης μνήμης που μόλις γεμίσαμε σαν παράμετρος στην κλήση υπολογισμού της ευκλείδιας απόστασης.
#my_sourcefile(
  "../a3/code_parts_display/shared_find_nearest_cluster.cu",
  lang:"cpp",
  highlighted:(13,14,15,16,17,18,19,20,21,34,37),
  highlight-color:  rgb(85, 0, 170).lighten(60%)
)
\
Το μέγεθος της διαμοιραζόμενης μνήμης χρειάζεται να δηλωθεί στην κλήση του GPU kernel. Η διαμοιραζόμενη μνήμη θα έχει numClusters × numCoords πραγματικούς αριθμούς. Οπότε:
\
const unsigned int clusterBlockSharedDataSize = numClusters \* numCoords \* sizeof(double);
\
Οι υπόλοιπες διαδικασίες αντιγραφής και μετατροπής των clusters παραμένουν ίδιες με την transpose εκδοχή.
\
Επαναλάβαμε τις μετρήσεις για την shared εκδοχή και παρακάτω παρουσιάζεται το διάγραμμα speedup σε σύγκριση με τις υπόλοιπες εκδοχές.
\
#align(center)[
  #image("../a3/plots_scraping/shared_transpose_naive_speedup.png",width:85%)
]
\
Παρατηρείται μια πτώση επίδοσης για blocksize > 64 και προφανώς, επειδή δεν είναι πολλαπλάσιο του 32 και σπαταλάει πόρους, το blocksize 238 έχει την χειρότερη επίδοση. Υπάρχουν 3 πιθανές εξηγήσεις για αυτό:
\
1)Καθυστέρηση λόγω δυσκολίας συγχρονισμού πολλών threads μέσα στο thread block\
2)Η διαμοιραζόμενη μνήμη δέχεται υπερβολικά κοινά αιτήματα από τα threads\
3)Με μεγάλα blocksizes υπάρχουν λίγα thread blocks ανά SM, δεν είναι τόσο ισομερώς κατανημεμημένα, οπότε δεν γίνεται η καλύτερη αξιοποίηση των πόρων.

#pagebreak()

== Σύγκριση υλοποιήσεων / bottleneck Analysis
\
Για καλύτερη ανάλυση των επιμέρους υλοποιήσεων ακολουθεί ένα διάγραμμα ανάλυσης των επιμέρους χρόνων CPU, GPU, transfers ανά επανάληψη. 
\
#align(center)[
  #image("../a3/plots_scraping/shared_gpu_cpu_comparison.png",width:85%)
]
Όπως είναι λογικό οι χρόνοι μεταφορών και της CPU είναι σχεδόν ίδιοι μεταξύ των υλοποιήσεων. Παρατηρείται πως στα blocksizes με τις καλύτερες επιδόσεις (32, 64), ο χρόνος εκτέλεσης στην GPU είναι σημαντικά μικρότερος από το χρόνο εκτέλεση στην CPU για τον υπολογισμό των καινούριων clusters. Οπότε, μια ιδέα θα ήταν να μεταφερθεί όλος ο φόρτος των επαναλήψεων στην GPU, ακόμη και να μην είναι ιδανικό για GPU, όπως θα δούμε και παρακάτω. 
\
\
Επαναλάβαμε τις μετρήσεις για όλες τις εκδοχές για {Size, Coords, Clusters, Loops} = {1024, 2, 64, 10} για τα πιθανά blocksizes. Το μέγεθος του προβλήματος παραμένει το ίδιο, αλλά μειώθηκε ο αριθμός των συντεταγμένων από 32 σε 2, οπότε αυξήθηκε αντίστοιχα ο αριθμός των αντικειμένων, άρα και των thread blocks. Ακολουθούν τα διαγράμματα για speedup και ανάλυση χρόνου επανάληψης για το καινούριο configuration.
\
#align(center)[
  #image("../a3/plots_scraping/bottleneck_coords_2_speedup.png",width:75%)
]
#align(center)[
  #image("../a3/plots_scraping/bottleneck_coords_2_gpu_cpu_comparison.png",width:85%)
]
\
Καθώς οι συντεταγμένες είναι μόνο 2, ένα cache line περιέχει παραπάνω από 1 συντεταγμένη για όλα τα cluster. Σε αυτήν την περίπτωση υπάρχει παρόμοιο locality και χωρίς διαμοιραζόμενη μνήμη, οπότε οι tranpose, shared διαφέρουν ελάχιστα μεταξύ τους στις επιδόσεις. Η shared έχει χειρότερη επίδοση σε μεγάλα blocksizes καθώς η ζήτηση για τα cache lines που πλέον είναι λιγότερα αυξάνεται σημαντικά με αποτέλεσμα η μνήμη να μην μπορεί να καλύψει την ζήτηση με την ίδια ταχύτητα. 
\
Η παρούσα shared υλοποίηση δεν είναι κατάλληλη για την επίλυση του kmeans για arbitrary configurations. Για να λειτουργήσει σωστά η ιδέα της διαμοιραζόμενης μνήμης δεν πρέπει τα cache lines να περιέχουν υπερβολικά μικρά δεδομένα, όπως είδαμε και στο copied clusters με first-touch policy και την βελτίωση με το numa-aware. 
\
\
Bonus1: σε όλες τις περιπτώσεις (και στις επόμενες υλοποιήσεις) η cudaOccupancyMaxPotentialBlockSize επιστρέφει 1024 που είναι το μέγιστο blocksize. Για το configuration με τις πολλές συντεταγμένες (32) αυτή είναι ίσως η χειρότερη επιλογή. Ενώ για τις λίγες συντεταγμένες (2) το 1024 είναι το κατάλληλο blocksize μόνο για την transpose εκδοχή και για τις υπόλοιπες ή έχει παρόμοια επίδοση με άλλα blocksizes ή χειρότερη. Επειδή η συνάρτηση δεν λαμβάνει υπόψιν το μέγεθος του προβλήματος, δεν δύναται να δώσει κατάλληλο blocksize για όλες τις λύσεις. Δυστυχώς όμως, δεν δίνει κατάλληλο blocksize για κανένα από τα 2 configurations.
#pagebreak()

== Full-offload (All-GPU)
\
Υπάρχουν πολλοί τρόποι να γίνει η υλοποίηση της update_centroid. Επιλέξαμε να κάνουμε την πρόσθεση των συντεταγμένων για τα καινούρια clusters στην find_nearest_cluster με atomicAdds, ώστε να αξιοποιηθεί ο ισομερισμός της συνολικής δουλειάς. Επειδή ο αριθμός των clusters × τον αριθμό των συντεταγμέων είναι επαρκώς μεγάλος για τις περισσότερες περιπτώσεις, τα collisions που θα χρειαστούν όντως συγχρονισμό είναι πολύ λιγότερα απ' όσα φαίνονται αρχικά. Ακόμη, ενημερώνεται με atomicAdd και το μέγεθος του cluster που είναι το πιο κοντινό σε κάθε σημείο.
\
Στην update controids κάθε thread αναλαμβάνει μία μόνο συντεταγμένη ενός νέου cluster και την διαιρεί με το μέγεθός του. Έπειτα, μηδενίζει την αντίστοιχη συντεταγμένη των, υπολογισμένων από την find_nearest_cluster, clusters ώστε να μπορεί να ξαναξεκινήσει επανάληψη το do-while από την αρχή σωστά. Χρειάζεται όμως να μηδενιστούν και τα μεγέθη αυτών των clusters. Αυτό μπορεί να γίνει μόνο αφού διαιρεθούν όλες οι συντεταγμένες, οπότε χρειάζεται συγχρονισμός των νημάτων και στο τέλος να μηδενιστούν τα μεγέθη. Σημειώνεται πως επειδή η δομή είναι do-while και όχι απλό while, χρειάζεται με την cudaMemSet να μηδενιστούν αρχικά τα devicenewClusters, ώστε οι προσθέσεις να είναι valid στην πρώτη κλήση του kernel find_nearest_cluster.
#my_sourcefile(
  "../a3/code_parts_display/all_gpu_calculations.cu",
  lang:"cpp",
  highlighted:(9,10,61,62,63,76,78,85,87,89,91,92,93),
  highlight-color:  rgb(85, 0, 170).lighten(60%)
)
\
Το kernel find_nearest_cluster χρειάζεται να κληθεί με περισσότερες παραμέτρους και ίδιο μέγεθος shared memory, ενώ η κλήση του kernel update_centroid δεν χρειάζεται καθόλου shared memory.
#my_sourcefile(
  "../a3/code_parts_display/all_gpu_do_while.cu",
  lang:"cpp",
  highlighted:(12,13,14,36,37),
  highlight-color:  rgb(85, 0, 170).lighten(60%)
)
\
Επαναλάβαμε τις μετρήσεις για όλες τις εκδοχές για τα 2 διαφορετικά configurations και παρακάτω παρουσιάζονται τα διαγράμματα για το speedup και την ανάλυση χρόνου εκτέλεσης ανά επανάληψη, ώστε να φαίνονται καλύτερα οι επιδόσεις των διαφόρων εκδοχών.
\
#align(center)[
  #image("../a3/plots_scraping/all_gpu_coords_32_speedup.png",width: 85%)
  #image("../a3/plots_scraping/all_gpu_coords_2_speedup.png",width: 85%)
  #image("../a3/plots_scraping/all_gpu_coords_32_comparison.png",width: 85%)
  #image("../a3/plots_scraping/all_gpu_coords_2_comparison.png",width: 85%)
]
\
1) Από τα διαγράμματα speedup παρατηρείται πως η Full-offload εκδοχή έχει καλύτερες επιδόσεις στο configuration με τις πολλές συντεταγμένες (32) ενώ έχει πολύ καλύτερες επιδόσεις στο configuration με τις λίγες συντεταγμένες (2).\
2) Το blocksize για τις πολλές συντεταγμένες (32) έχει παρόμοια επιρροή με τις υπόλοιπες εκδοχές και έχει βέλτιστη επίδοση για τα 2 μικρότερα block sizes. Όπως έχει προαναφερθεί, τα μικρά blocksizes έχουν την μέγιστη ευελιξία για το scheduling και είναι λογικό να έχουν καλύτερες επιδόσεις, αφού τα δεδομένα είναι πλήρως ανεξάρτητα. Το blocksize για τις λίγες συντεταγμένες (2) δεν επηρεάζει εμφανώς την επίδοση, με εξαίρεση το 48 που λόγω half warps και ότι δεν αποτελεί bottleneck η μνήμη, δεν αξιοποιεί πλήρως τους πόρους της GPU.\
3) Το κομμάτι update_centroids έχει πολύ χαμηλό computational intensity, οπότε δεν είναι ιδανικό για GPUs. Όμως, είναι πλήρως παραλληλοποιήσιμο, γι' αυτό έχουμε και σαφές speedup σε σχέση με την χρήση CPU. Μέσω της ανάλυσης χρόνου εκτέλεσης ανά επανάληψη, μπορεί να φανεί πως ο υπολογισμός των καινούριων clusters στην GPU αύξησε ελάχιστα τον χρόνο εκτέλεσης της GPU, ενώ προφανώς μηδενίστηκε ο χρόνος εκτέλεσης της CPU. Σε αυτό οφείλεται η διαφορά επίδοσης, απλά στο speedup συνυπολογίζεται και ο χρόνος allocation και αρχικής μεταφοράς. Γι' αυτό δεν υπάρχει ανάλογο speedup συγκριτικά.\
4) Στο configuration με τις λίγες συντεταγμένες (2) ο χρόνος μεταφοράς έχει σημαντικό ποσοστό του συνολικού χρόνου μιας επανάληψης. Καθώς η εκδοχή Full-offload αποφεύγει και τις μεταφορές μέσα στην επανάληψη, το speedup είναι ακόμη μεγαλύτερο.\

#pagebreak()

== Bonus 2: Delta Reduction (All-GPU)
\
Για την υλοποιήση του δενδρικού delta reduction αρχικά χρειάζεται περισσότερη διαμοιραζόμενη μνήμη. Κάθε thread στο thread block πρέπει να έχει τον δικό του delta και μετά να γίνει το reduction. Γι' αυτό χρειάζεται ένας πίνακας από delta και για κάθε thread αντιστοιχεί μια θέση στον πίνακα με όρισμα το local id του.
\
Κάθε thread αφού βρει το καινούριο κοντινότερο cluster ελέγχει αν είναι το ίδιο με πριν. Αν είναι, τότε βάζει το δικό του delta να είναι 0.0, αλλιώς 1.0. Έπειτα, χρειάζεται συγχρονισμός των threads, πριν εκτελεστεί το reduction ώστε να έχουν υπολογιστεί όλα τα επιμέρους delta.
\
Το δενδρικό reduction ακολουθεί την λογική ότι σε κάθε επανάληψη οι μισοί προσθέτουν στο δικό τους delta, το delta των υπολοίπων. Οπότε οι πρώτοι μισοί εκτελούν \
delta[local_id] += delta[local_id + size] και σε κάθε επανάληψη μειώνεται το μέγεθος κατά 2, εξού και δέντρο. Μεταξύ των επαναλήψεων χρειάζεται συγχρονισμός των νημάτων για να έχουν υπολογιστεί τα αποτελέσματα όλων των προσθέσεων. Στο τέλος, το συνολικό αποτέλεσμα θα είναι στο πρώτο thread του thread block, το οποίο χρειάζεται να κάνει μόνο 1 atomicAdd στο global delta.
#my_sourcefile(
  "../a3/code_parts_display/delta_reduction_find_nearest_cluster.cu",
  lang:"cpp",
  highlighted:(14,51,52,53,54,55,56,75,76,77,78,79,80,81,82,83),
  highlight-color:  rgb(85, 0, 170).lighten(60%)
)
\
Τέλος, η μόνη αλλαγή που χρειάζεται στο υπόλοιπο πρόγραμμα είναι η αύξηση του μεγέθους της διαμοιραζόμενης μνήμης κατά blocksize πραγματικούς. Οπότε:\
const unsigned int clusterBlockSharedDataSize = numClusters \* numCoords \* sizeof(double) + numThreadsPerClusterBlock \* sizeof(double);\
\
\
Επαναλάβαμε τις μετρήσεις για την delta reduction εκδοχή και παρακάτω παρουσιάζονται τα διαγράμματα speedup και ανάλυσης χρόνου εκτέλεσης ανά επανάληψη σε σύγκριση με την Full-offload εκδοχή για τα 2 διαφορετικά configurations. 
\
#align(center)[
  #image("../a3/plots_scraping/delta_reduction_coords_32_speedup.png",width:85%)
  #image("../a3/plots_scraping/delta_reduction_coords_2_speedup.png",width:85%)
  #image("../a3/plots_scraping/delta_reduction_coords_32_comparison.png",width:85%)
  #image("../a3/plots_scraping/delta_reduction_coords_2_comparison.png",width:85%)
]
\
Η επίδοση της delta reduction εκδοχής είναι χειρότερη από την απλή Full-offload στο configuration με τις πολλές συντεταγμένες (32). Αυτό θα μπορούσε να εξηγηθεί ως προς τον παραπάνω συγχρονισμό που απαιτεί η delta reduction εκδοχή. Επειδή θα γίνουν ούτως ή άλλως 32 atomicAdds για τις συντεταγμένες, δεν θα κάνουν όλα τα threads, που έχουν διαφορετικό καινούριο cluster, ταυτόχρονα atomicAdd στο delta. Δεν θα είναι τόσο ταυτόχρονα στο χρόνο τα atomicAdds, καθώς θα υπάρχουν διαφορετικές μικρές καθυστερήσεις λόγω των 32 προηγούμενων atomicAdds. Έτσι, όχι μόνο δεν υπάρχει βελτίωση, αλλά υπάρχει και μια μικρή επιπρόσθετη καθυστέρηση.
\
Σε αντίθεση, στο configuration με τις λίγες συντεταγμένες (2) καθώς τα atomicAdds έχουν όντως πολλές κοντινές χρονικά εκτελέσεις και απαιτείται όντως συγχρονισμός, η εκδοχή του delta reduction έχει όντως βελτίωση επίδοσης. Συγκριτικά, θέλει περίπου 25% λιγότερο χρόνο ανά loop για το βέλτιστο blocksize σε σχέση με την απλή Full-offload εκδοχή.
\
Το blocksize έχει διαφορετικό ρόλο μόνο στο configuration με τις λίγες συντεταγμένες (2), καθώς χρειάζεται το blocksize να είναι επαρκώς μεγάλο ώστε η λογαριθμική του πολυπλοκότητα να είναι καλύτερη από την γραμμική που έχει η απλή Full-offload. Σε μικρά blocksizes, η βελτίωση στην επίδοση που υπάρχει είναι μικρότερη καθώς η σταθερά αυτής της πολυπλοκότητας είναι αρκετά μεγάλη. 
\
== Παράρτημα 
\
Για την δημιουργία των γραφικών παραστάσεων χρησιμοποίηθηκαν αρκετοί διαφορετικοί κώδικες σε Python. Ενδεικτικά για την σύγκριση όλων των εκδοχών εκτός της delta reduction, χρησιμοποιώθηκε ο παρακάτω κώδικας:
#my_sourcefile("../a3/plots_scraping/all_gpu_coords_2.py")

#pagebreak()

= Παραλληλοποιήση αλγορίθμων με χρήση MPI 
\
== Άλλη μια παραλληλοποίηση του K-means
\
Σκοπός αυτής της άσκησης είναι η υλοποίηση του αλγορίθμου Kmeans πρωτόκολλο ανταλλαγής μηνυμάτων με χρήση του MPI.
\
\
Ο σκελετός του αλγορίθμου είναι ίδιος με την υλοποίηση copied clusters, που το κάθε νήμα/διεργασία έχει αντίγραφα των παλιών clusters και έχει δικό του τοπικό πίνακα. Το τελικό βήμα της κάθε επανάληψης είναι το reduction των καινούριων συντεταγμένων των clusters. Σε μοντέλο κοινού χώρου διευθύνσεων, χρειάζεται μόνο reduction αφού υπάρχει κοινός πίνακας. Σε μοντέλο ανταλλαγής μηνυμάτων, χρειάζεται να γίνει το reduction των καινούριων συντεταγμένων αλλά και broadcast σε όλους, καθώς δεν υπάρχει κοινός χώρος διευθύνσεων. Αυτό μπορεί να γίνει με την χρήση των εντολών MPI_Reduce στην διεργασία με rank 0 και MPI_Bcast σε όλες τις διεργασίες. 
\
Η διαδικασία αυτή χρειάζεται να γίνει και για τις συντεταγμένες και για τα μεγέθη των καινούριων clusters, καθώς και για την μεταβλητή delta για κατάλληλο έλεγχο τερματισμού του αλγορίθμου.
\
\
#my_sourcefile(
  "../a4/kmeans/src/kmeans.c",
  lang:"cpp",
  highlighted:(109,110,112,113,127,128),
  highlight-color:  rgb(85, 0, 170).lighten(60%)
)
\
Χρειάζεται αρχικά κατάλληλος διαμοιρασμός των objects στις διεργασίες. Η λογική που ακολουθήσαμε ήταν να δώσουμε το πηλίκο, της διαίρεσης πλήθους objects δια size - 1, σε size - 1 διεργασίες και το υπόλοιπο στην τελευταία. Έτσι, καταλήπτουμε την γενική περίπτωση που δεν θα πάρει κάθε διεργασία τον ίδιο αριθμό αντικειμένων.
\
Το πρώτο rank έχει στην μεταβλητή rank_numObjs την τιμή του πηλίκου της διαίρεσης. Οπότε, για όλες τις διεργασίες εκτός την τελευταίας το send count είναι αυτή η τιμή επί τον αριθμό των συντεταγμέων κάθε αντικειμένου. Αντίστοιχα, για την τελευταία διεργασία είναι το πλήθος αντικειμένων - ό,τι πήραν οι προηγούμενες, ακριβώς δηλαδή ό,τι περισσεύει. Το displs[i] είναι displs[i-1] + τα sendcounts της διεργασίας i.
\
Μετά τον υπολογισμό των send_coutns, displs, γίνονται broadcast ώστε να μπορεί να γίνει κατάλληλο scatter. Με την εντολή MPI_Scatterv μπορεί να γίνει scatter με διαφορετικό αριθμό αντικειμένων σε κάθε διεργασία. Ως παράμετροι χρησιμοποιούνται οι πίνακες send_counts, displs που υπολογίσαμε παραπάνω.
\
\
#my_sourcefile(
  "../a4/kmeans/src/file_io.c",
  lang:"cpp",
  highlighted:(27,28,29,30,31,32,33,45,46,47,48,49,50,51,52,58,59,82),
  highlight-color:  rgb(85, 0, 170).lighten(60%)
)
\
Για την main.c χρειάζεται να κάνουμε ένα αρχικό broadcast θέσεων, καθώς και κατάλληλη συλλογή της τελικής λύσης του αλγορίθμου.
\
Πιο συγκεκριμένα, αρχικά χρειάζεται να καλέσουμε την συνάρτηση kmeans με rank_numObjs ως παράμετρου, όπου εκεί είναι αποθηκευμένα πόσα αντικείμενα πρέπει να εξετάσει η κάθε διεργασία ξεχωριστά. 
\
Οι πίνακες recv_counts, displs ακολουθούν την ίδια ακριβώς λογική με τους send_counts, displs στο αρχείο file_io.c. Όμως, εδώ χρειάζεται συλλογή μόνο για το membership του κάθε αντικειμένου και όχι για όλες τις συντεταγμένες του. Άρα, για όλες τις διεργασίες είναι rank_numObjs ακέραιοι αριθμοί. Τέλος, γίνεται η συλλογή στην διεργασία με rank 0, με την εντολή MPI_Gatherv με παράμετρους τους πίνακες που υπολογίστηκαν παραπάνω.
\
\
#my_sourcefile(
  "../a4/kmeans/src/main.c",
  lang:"cpp",
  highlighted:(117,128,149,150,151,152,153,154,155,156,162,163,167),
  highlight-color:  rgb(85, 0, 170).lighten(60%)
)
\
Πραγματοποιήσαμε μετρήσεις για {Size, Coords, Clusters, Loops} = {256, 16, 32, 10} για 1, 2, 4, 8, 16, 32 και 64 MPI διεργασίες. Παρακάτω παρουσιάζονται τα διαγράμματα speedup και ανάλυσης χρόνου εκτέλεσης ως προς τον αριθμό διεργασιών για το mpi, αλλά και συγκριτικά με την υλοποίηση numa-aware με Openmp από την δεύτερη άσκηση.
\
#align(center)[
  #image("../a4/kmeans/results/execution_time_comparison.svg",width:85%)
  #image("../a4/kmeans/results/speedup_comparison.svg",width:85%)
]
\
Παρατηρείται πως η υλοποίηση στο μοντέλο ανταλλαγής μηνυμάτων έχει τέλεια κλιμάκωση στο πλήθος διεργασιών, όμως είναι αρκετά πιο αργή σε σχέση με την υλοποίηση numa-aware με Openmp. Αυτό συμβαίνει καθώς η numa-aware εκμεταλλεύται πλήρως τον κοινό χώρο διευθύσεων (με χρήση thread pinning και preload αντικειμένων στις caches), ενώ οι διεργασίες στο mpi χρειάζεται να πραγματοποιούν επικοινωνία μεταξύ τους. Επίσης, η υλοποίηση στο μοντέλο ανταλλαγής μηνυμάτων έχει μεγαλύτερο χρόνο εκτέλεσης από την υλοποίηση στο μοντέλο κοινού χώρου διευθύνσεων, καθώς η επικοινωνία μεταξύ των διεργασιών είναι πιο ακριβή.

#pagebreak()

=== Red-Black SOR 
Για την παραλληλοποίηση της μεθόδου Red-Black με SOR υλοποιήσαμε *3 διαφορετικές εκδοχές: Blocking, Non-Blocking και Overlapping* οι οποίες περιγράφονται παρακάτω: 

=== 1) Βlocking version (Sendrecv)
Αρχικά, χρησιμοποιήθηκε ο ίδιος σκελετός με την υλοποιήση του Jacobi με την διαφορά ότι ο υπολογισμός γίνεται σε 2 φάσεις (μία για τα μαύρα και μία για τα κόκκινα σημεία). Παρατηρούμε ότι απαιτείται ξανά η χρήση των MPI μηνυμάτων για την ανταλλαγή των συνοριακών γραμμών-στηλών μόλις  τελειώσει η φάση 1, (μαύρα σημεία) διότι η φάση 2 εκτελεί τον υπολογισμό πάνω στις ανανεωμένες τιμές του grid. 

#my_sourcefile("../a4/heat_transfer/src/mpi/mpi_red_black.c")

=== 2) Non-Blocking version (Isend/Irecv) και ανταλλαγή N/2 σημείων
Η προηγούμενη εκδοχοχή κάνει πολλές περιττές ανταλλαγές μηνυμάτων, συγκεκριμένα σε κάθε γύρο μεταφέρονται διπλάσα δεδομένα από όσα απαιτούνται. Γι' αυτό, δημιουργήσαμε ένα νέο datatype με διπλάσιο stride από πριν ώστε να skip-άρουμε τα κόκκινα σημεία όταν βρισκόμαστε στην φάση 1 (υπολογισμός μαύρων) και αντίστοιχα να skip-άρουμε τα μαύρα όταν βρισκόμαστε στην φάση 2 (υπολογισμός κόκκινων). 
Ακόμη, η αποστολή και λήψη των συνοριακών σημειών μπορεί να γίνει ταυτόχρονα προς όλες τις κατευθύνσεις εφόσον χρησιμοποιεί ξεχωριστά sockets για κάθε γείτονα-process. Οπότε η επιλογή των Isend/Irecv που είναι non-blocking επιταχύνουν την ανταλλαγή. Χρειάζεται προσοχή στην επιλογή των αρχικών διευθύνσεων των send και receive buffers ώστε τα dataypes που ορίσουμε να τοποθετούν τα δεδομένα σε άρτιες ή περιττές θέσεις αναλόγως την φάση που βρισκόμαστε (κατά σύμβαση even indices -> black point, odd indices -> red points). Πρωτού ξεκινήσουν οι υπλογισμοί, εξασφαλίζουμε με Waitall ότι όλες οι μεταφορές έχουν ολοκληρωθεί. 
Τέλος, ως επιπλεόν βελτιστοποίηση, τροποποιούμε τον kernel του RedBlack και για τις 2 φάσεις ώστε να γλιτώσουμε n^2 επαναλήψεις με περιττά brnaches. Συγκεκριμένα, για κάθε i επιλέγουμε το πρώτο j που δίνει άρτιο (ή περιττό) άθροισμα και έπειτα χρησιμοποιούμε βήμα 2 για το εσωτερικό loop. 

#my_sourcefile("../a4/heat_transfer/src/mpi/mpi_red_black_async.c")

=== 3) Οverlapping version 
Η κύρια ιδέα αυτής της εκδοχής είναι η ακόλουθη: Σε κάθε γύρο υπολογίζεται πρώτα η περίμετρος των μαύρων σημείων και στέλνεται αμέσως στους γείτονες με non-blocking τρόπο (Isend/Irecv) ώστε ταυτόχρονα να μπορεί να ξεκινήσει η υπολογισμός των εσωτερικών σημείων. Μόλις οι γείτονες λάβουν τις ανανεωμένες συνοριακές τιμές για τα μαύρα σημεία (Waitall), μπορούν να ξεκινήσουν τον υπολογισμό της περιμέτρου για τα κόκκινα σημεία. Ομοιώς, όταν ολοκληρωθεί στέλνεται με non-blocnikg τρόπο στους γείτονες και ταυτόχρονα ξεκινάει ο υπολογισμός των εσωτερικών κόκκινων σημείων. 
Η ορθότητα της μεθόδου βασίζεται στην εξής παρατήρηση: Σε κάθε γύρο (red | black) οι μοναδικές εξαρτήσεις εμφανίζονται στα συνοριακά σημεία μετά το τέλος της κάθε φάσης αφού στο υπόλοιπο grid οι τιμές έχουν ανανεωθεί από το ίδιο το process και μπορούν να ξαναχρησιμοποιηθούν αμέσως από αυτό στην επόμενη φάση. Επιπλέον, όλα τα σημεία του grid μπορούν να υπολογιστούν με οποιαδήποτε σειρά σε μία φάση, αφού γειτονικά cells δεν υπολογίζονται στον ίδια φάση. 
Οι πηρύνες υπολογισμών που χρησιμοποιόυνται ορίζονται ως inline συναρτήσεις και είναι οι βελτιστοποιημένες εκδοχές που περιγράφηκαν παραπάνω. 

#my_sourcefile("../a4/heat_transfer/src/mpi/mpi_red_black_overlapping.c")

Oι συγκρίσεις των χρόνων επικοινωνίας και συνολικών χρόνων εκτέλεσης μέχρι το σημείο της σύγκλισης για σταθερό grid μεγέθους 512x512 φαίνονται παρακάτω: 

#image("../a4/heat_transfer/results/parlab/red_black_comm.png")
#image("../a4/heat_transfer/results/parlab/red_black_toatl.png")


Η Overlapping εκδοχή έχει αμελητέο κόστος επικοινωνίας αφού ταυτόχρονα εκτελεί υπολογισμούς σε κάθε στάδιο του αλγορίθμου. Από το σημείο αυτό και έπειτα η εφαρμογή γίνεται εξ'ολοκληρού compute-bound και δεν βλέπουμε δυνατότητα περεταίρω βελτιστοποίησης. 
/ Σημείωση: Παρά τα optimisations στο computational part, δεν βλέπουμε μεγάλη μείωση στον χρόνο υπολογισμού (επειδή μεταγλωττίστηκαν όλες οι εκδόσεις με -Ο3 πιθανά ο compiler κάνει μόνος του κάποια από τα tricks που γράψαμε explicitly στις εκδόσεις 2 κ' 3).

Tα ατίστοιχα speedup plots καρτώντας σταθερό των αριθμό επαναλήψεων T=256 για τα 3 ζητούμενα μεγέθη grid παρατίθενται ακολούθως για λόγους σύγρκισης των μεθόδων:

#align(center)[
#image("../a4/heat_transfer/results/parlab/Speedup_(2048, 2048).png", width:70%)
#image("../a4/heat_transfer/results/parlab/Speedup_(4096, 4096).png", width:70%)
#image("../a4/heat_transfer/results/parlab/Speedup_(6144, 6144).png",width:70%)]